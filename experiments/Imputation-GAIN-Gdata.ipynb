{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIN: Missing data imputation using Generative Adversarial Nets\n",
    "\n",
    "**Generative Adversarial Network (GAN)**, is a special case of Adversarial Process where the components are neural networks. The first network called Generator ($G$) generates data (i.e., fake data), and the second network called Discriminator ($D$) tries to tell the difference between the real data and the fake data.\n",
    "\n",
    "**Generative Adversarial Imputation Network**, or **GAIN** in short, is a novel method for imputing missing data by adapting the well-known GAN framework.\n",
    "\n",
    "In GAIN, the Generator observes some elements of a read data vector, imputes the missing values conditioned on what is actually observed, and outputs a complete vector. The Discriminator then takes a complete vector and attempts to determine which elements were actually observed and which were imputed.\n",
    "\n",
    "**Reference**\n",
    "\n",
    "Jinsun Yoon, James Jordon, Mihaela van der Schaar, 2018. [GAIN: missing data imputation using Generative Adversarial Nets](http://proceedings.mlr.press/v80/yoon18a/yoon18a.pdf). Proceedings of the 35th International Conference on Machine Learning (ICML 2018), Stockholm, Sweden. [[supplementary materials](http://medianetlab.ee.ucla.edu/papers/ICML_GAIN_Supp.pdf)] [[Python code - GitHub](https://github.com/jsyoon0823/GAIN)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "The following experiment is supported by the [Urban traffic speed data set in Gaungzhou, China](https://zenodo.org/record/1205229)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "from tensorly import unfold\n",
    "\n",
    "tensor = scipy.io.loadmat('Guangzhou-data-set/tensor.mat')\n",
    "tensor = tensor['tensor']\n",
    "random_matrix = scipy.io.loadmat('Guangzhou-data-set/random_matrix.mat')\n",
    "random_matrix = random_matrix['random_matrix']\n",
    "random_tensor = scipy.io.loadmat('Guangzhou-data-set/random_tensor.mat')\n",
    "random_tensor = random_tensor['random_tensor']\n",
    "\n",
    "mat = unfold(tensor, 0)\n",
    "missing_rate = 0.2\n",
    "\n",
    "# =============================================================================\n",
    "### Set the random misssing (RM) scenario by:\n",
    "# binary_mat = unfold(np.round(random_tensor + 0.5 - missing_rate), 0)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "### Set the non-random missing (NM) scenario by:\n",
    "binary_tensor = np.zeros(tensor.shape)\n",
    "for i1 in range(tensor.shape[0]):\n",
    "    for i2 in range(tensor.shape[1]):\n",
    "        binary_tensor[i1,i2,:] = np.round(random_matrix[i1,i2] + 0.5 - missing_rate)\n",
    "binary_mat = unfold(binary_tensor, 0)\n",
    "# =============================================================================\n",
    "\n",
    "sparse_mat = np.multiply(mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = mat.T\n",
    "binary_mat = binary_mat.T\n",
    "sparse_mat = sparse_mat.T\n",
    "\n",
    "num_row = mat.shape[0]\n",
    "num_col = mat.shape[1]\n",
    "\n",
    "num_h1 = num_col\n",
    "num_h2 = num_col\n",
    "\n",
    "X0 = mat\n",
    "M0 = binary_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIN architecture\n",
    "\n",
    "1) For the purpose of training, we define the following `placeholder` for $X,~M,~H$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, num_col])\n",
    "M = tf.placeholder(tf.float32, shape = [None, num_col])\n",
    "H = tf.placeholder(tf.float32, shape = [None, num_col])\n",
    "New_X = tf.placeholder(tf.float32, shape = [None, num_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Define the xavier initialization.\n",
    "\n",
    "**Discriminator**: $\\Theta^{(D)} =\\left\\{W_1^{(D)},b_1^{(D)},W_2^{(D)},b_2^{(D)},W_3^{(D)},b_3^{(D)}\\right\\}$\n",
    "\n",
    "**Generator**: $\\Theta^{(G)} =\\left\\{W_1^{(G)},b_1^{(G)},W_2^{(G)},b_2^{(G)},W_3^{(G)},b_3^{(G)}\\right\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape = size, stddev = xavier_stddev)\n",
    "\n",
    "D_W1 = tf.Variable(xavier_init([2 * num_col, num_h1]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape = [num_h1]))\n",
    "\n",
    "D_W2 = tf.Variable(xavier_init([num_h1, num_h2]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape = [num_h2]))\n",
    "\n",
    "D_W3 = tf.Variable(xavier_init([num_h2, num_col]))\n",
    "D_b3 = tf.Variable(tf.zeros(shape = [num_col]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_W3, D_b1, D_b2, D_b3]\n",
    "\n",
    "G_W1 = tf.Variable(xavier_init([2 * num_col, num_h1]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape = [num_h1]))\n",
    "\n",
    "G_W2 = tf.Variable(xavier_init([num_h1, num_h2]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape = [num_h2]))\n",
    "\n",
    "G_W3 = tf.Variable(xavier_init([num_h2, num_col]))\n",
    "G_b3 = tf.Variable(tf.zeros(shape = [num_col]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_W3, G_b1, G_b2, G_b3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator and Discriminator networks implementation\n",
    "\n",
    "We implement the **Generator** and **Discriminator** networks using the following functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(New_X, M):\n",
    "    inputs = tf.concat(axis = 1, values = [New_X, M])\n",
    "    G_h1 = tf.nn.elu(tf.matmul(inputs, G_W1) + G_b1)\n",
    "    G_h2 = tf.nn.elu(tf.matmul(G_h1, G_W2) + G_b2)\n",
    "    G_prob = tf.nn.elu(tf.matmul(G_h2, G_W3) + G_b3)\n",
    "    return G_prob\n",
    "\n",
    "def discriminator(New_X, H):\n",
    "    inputs = tf.concat(axis = 1, values = [New_X, H])\n",
    "    D_h1 = tf.nn.elu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    D_h2 = tf.nn.elu(tf.matmul(D_h1, D_W2) + D_b2)\n",
    "    D_logit = tf.matmul(D_h2, D_W3) + D_b3\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAIN training\n",
    "\n",
    "#### Generator:\n",
    "\n",
    "The generator, $G$, takes $\\tilde{X}$ and $M$ as inputs and outpus an imputed vector. The random variable $\\hat{X}$ is defined in the following way:\n",
    "\n",
    "$$\\hat{X}=M\\circledast\\tilde{X}+(1-M)\\circledast G\\left(\\tilde{X},M\\right)$$\n",
    "where $\\circledast$ denotes Hadamard product (element-wise multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 5\n",
    "\n",
    "G_sample = generator(New_X, M)\n",
    "New_X_hat = New_X * M + G_sample * (1 - M)\n",
    "D_prob = discriminator(New_X_hat, H)\n",
    "\n",
    "D_loss1 = -tf.reduce_mean(M * tf.log(D_prob + 1e-8) + (1 - M) * tf.log(1. - D_prob + 1e-8))\n",
    "G_loss1 = -tf.reduce_mean((1 - M) * tf.log(D_prob + 1e-8))\n",
    "MSE_train_loss = tf.reduce_mean((M * New_X - M * G_sample) ** 2) / tf.reduce_mean(M)\n",
    "\n",
    "D_loss = D_loss1\n",
    "G_loss = G_loss1 + alpha * MSE_train_loss\n",
    "\n",
    "MSE_test_loss = tf.reduce_mean(((1 - M) * X - (1 - M) * G_sample) ** 2) / tf.reduce_mean(1 - M)\n",
    "\n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 500\n",
      "Train_loss: 8.653\n",
      "Test_loss: 9.767\n",
      "\n",
      "Iter: 1000\n",
      "Train_loss: 8.09\n",
      "Test_loss: 9.143\n",
      "\n",
      "Iter: 1500\n",
      "Train_loss: 7.984\n",
      "Test_loss: 9.148\n",
      "\n",
      "Iter: 2000\n",
      "Train_loss: 7.915\n",
      "Test_loss: 9.174\n",
      "\n",
      "Iter: 2500\n",
      "Train_loss: 6.609\n",
      "Test_loss: 7.984\n",
      "\n",
      "Iter: 3000\n",
      "Train_loss: 5.627\n",
      "Test_loss: 6.947\n",
      "\n",
      "Iter: 3500\n",
      "Train_loss: 5.018\n",
      "Test_loss: 6.435\n",
      "\n",
      "Iter: 4000\n",
      "Train_loss: 4.963\n",
      "Test_loss: 6.507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "max_iter = 4000\n",
    "for iter in range(1, max_iter + 1):\n",
    "    X_mb = X0\n",
    "    M_mb = M0\n",
    "    H_mb = M0\n",
    "    New_X_mb = M_mb * X_mb\n",
    "    \n",
    "    _, D_loss_curr = sess.run([D_solver, D_loss1], feed_dict = {M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "    _, G_loss_curr, MSE_train_loss_curr, MSE_test_loss_curr = sess.run([G_solver, G_loss1, MSE_train_loss, MSE_test_loss], \n",
    "                                                                       feed_dict = {X: X_mb, M: M_mb, New_X: New_X_mb, H: H_mb})\n",
    "    \n",
    "    if iter % 500 == 0:\n",
    "        sample = sess.run(G_sample, feed_dict = {X: X0, M: M0, New_X: M0 * X0})\n",
    "        smaple = M * X + (1 - M) * sample\n",
    "        \n",
    "        print('Iter: {}'.format(iter))\n",
    "        print('Train_loss: {:.4}'.format(np.sqrt(MSE_train_loss_curr)))\n",
    "        print('Test_loss: {:.4}'.format(np.sqrt(MSE_test_loss_curr)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE = 0.127649\n",
      "RMSE = 6.55002\n"
     ]
    }
   ],
   "source": [
    "ind = np.argwhere((mat > 0) & (sparse_mat == 0))\n",
    "mape = np.sum(abs(mat[ind[:, 0], ind[:, 1]] - sample[ind[:, 0], ind[:, 1]]) / mat[ind[:, 0], ind[:, 1]] / len(ind))\n",
    "rmse = np.sqrt(np.sum((mat[ind[:, 0], ind[:, 1]] - sample[ind[:, 0], ind[:, 1]]) ** 2) / len(ind))\n",
    "print('MAPE = {:.6}'.format(mape))\n",
    "print('RMSE = {:.6}'.format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment results** of missing data imputation using GAIN:\n",
    "\n",
    "|  scenario |`alpha`| `num_h1` | `num_h2` | `max_iter` | noise | MAPE | RMSE |\n",
    "|:----------|------:|---------:|---------:|-----------:|------:|-----:|-----:|\n",
    "|**0.2, RM**|   10 | `num_col` | `num_col` | 5000 | `None` | 0.1031 | 4.8601 |\n",
    "|**0.2, RM**|   10 | `num_col` | `num_col` | 4000 | `None` | 0.1034 | **4.6718** |\n",
    "|**0.2, RM**|    5 | `num_col` | `num_col` | 5000 | `None` | 0.1062 | 5.5075 |\n",
    "|**0.2, RM**|    5 | `num_col` | `num_col` |10000 | `None` | 0.1105 | 6.1929 |\n",
    "|**0.2, RM**|   20 | `num_col` | `num_col` | 4000 | `None` | 0.1021 | 4.5394 |\n",
    "|**0.2, RM**|   20 | `num_col` | `num_col` | 5000 | `None` | 0.1071 | 5.4990 |\n",
    "|**0.4, RM**|   10 | `num_col` | `num_col` | 4000 | `None` | 0.1063 | **5.1776** |\n",
    "|**0.4, RM**|   10 | `num_col` | `num_col` | 5000 | `None` | 0.1114 | 5.8402 |\n",
    "|**0.2, NM**|   10 | `num_col` | `num_col` | 4000 | `None` | 0.1276 | **6.5500** |\n",
    "|**0.4, NM**|   10 | `num_col` | `num_col` | 5000 | `None` | 0.1813 | 10.6302 |\n",
    "|**0.4, NM**|    5 | `num_col` | `num_col` | 5000 | `None` | 0.1501 | 7.1339 |\n",
    "|**0.4, NM**|    5 | `num_col` | `num_col` | 5000 | `None` | 0.1648 | 7.5626 |\n",
    "|**0.4, NM**|    5 | `num_col` | `num_col` | 1000 | `None` | 0.1382 | **6.9947** |\n",
    "|**0.4, NM**|   20 | `num_col` | `num_col` | 5000 | `None` | 0.1560 | 7.2674 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
