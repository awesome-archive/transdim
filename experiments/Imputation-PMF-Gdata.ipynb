{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def pmf(train_vec, val_vec, num_feat, epsilon, _lambda, momentum, maxepoch, num_batches, batch_size):   \n",
    "    # mean subtraction\n",
    "    mean_inv = np.mean(train_vec[:,2])\n",
    "        \n",
    "    pairs_tr = train_vec.shape[0]\n",
    "    pairs_va = val_vec.shape[0]\n",
    "        \n",
    "    # 1-p-i, 2-m-c\n",
    "    num_inv = int(max(np.amax(train_vec[:,0]), np.amax(val_vec[:,0]))) + 1\n",
    "    num_com = int(max(np.amax(train_vec[:,1]), np.amax(val_vec[:,1]))) + 1\n",
    "\n",
    "    incremental = False\n",
    "    if ((not incremental) or (w_C is None)):\n",
    "        # initialize\n",
    "        epoch = 0\n",
    "        w_C = 0.1 * np.random.randn(num_com, num_feat)\n",
    "        w_I = 0.1 * np.random.randn(num_inv, num_feat)\n",
    "            \n",
    "        w_C_inc = np.zeros((num_com, num_feat))\n",
    "        w_I_inc = np.zeros((num_inv, num_feat))\n",
    "\n",
    "    while epoch < maxepoch:\n",
    "        epoch += 1\n",
    "\n",
    "        # Shuffle training truples\n",
    "        shuffled_order = np.arange(train_vec.shape[0])\n",
    "        np.random.shuffle(shuffled_order)\n",
    "\n",
    "        # Batch update\n",
    "        for batch in range(num_batches):\n",
    "            # print \"epoch %d batch %d\" % (epoch, batch+1)\n",
    "            batch_idx = np.mod(np.arange(batch_size * batch,\n",
    "                                         batch_size * (batch+1)),\n",
    "                               shuffled_order.shape[0])\n",
    "\n",
    "            batch_invID = np.array(train_vec[shuffled_order[batch_idx], 0], dtype='int32')\n",
    "            batch_comID = np.array(train_vec[shuffled_order[batch_idx], 1], dtype='int32')\n",
    "\n",
    "            # Compute Objective Function\n",
    "            pred_out = np.sum(np.multiply(w_I[batch_invID,:], \n",
    "                                          w_C[batch_comID,:]),\n",
    "                              axis=1) # mean_inv subtracted\n",
    "\n",
    "            rawErr = pred_out - train_vec[shuffled_order[batch_idx], 2] + mean_inv\n",
    "\n",
    "            # Compute gradients\n",
    "            Ix_C = 2 * np.multiply(rawErr[:, np.newaxis], w_I[batch_invID,:]) \\\n",
    "                    + _lambda * w_C[batch_comID,:]\n",
    "            Ix_I = 2 * np.multiply(rawErr[:, np.newaxis], w_C[batch_comID,:]) \\\n",
    "                    + _lambda * w_I[batch_invID,:]\n",
    "            \n",
    "            dw_C = np.zeros((num_com, num_feat))\n",
    "            dw_I = np.zeros((num_inv, num_feat))\n",
    "\n",
    "            # loop to aggreate the gradients of the same element\n",
    "            for i in range(batch_size):\n",
    "                dw_C[batch_comID[i],:] += Ix_C[i,:]\n",
    "                dw_I[batch_invID[i],:] += Ix_I[i,:]\n",
    "\n",
    "\n",
    "            # Update with momentum\n",
    "            w_C_inc = momentum * w_C_inc + epsilon * dw_C / batch_size\n",
    "            w_I_inc = momentum * w_I_inc + epsilon * dw_I / batch_size\n",
    "\n",
    "\n",
    "            w_C = w_C - w_C_inc\n",
    "            w_I = w_I - w_I_inc\n",
    "\n",
    "        # Compute train error\n",
    "        train_out = np.sum(np.multiply(w_I[np.array(train_vec[:, 0], dtype='int32'), :],\n",
    "                                       w_C[np.array(train_vec[:, 1], dtype='int32'), :]), axis = 1)\n",
    "        error_train = train_out - train_vec[:, 2] + mean_inv\n",
    "        train_rmse = LA.norm(error_train)/np.sqrt(len(train_vec))\n",
    "        # Compute validation error\n",
    "        test_out = np.sum(np.multiply(w_I[np.array(val_vec[:, 0], dtype='int32'), :],\n",
    "                                      w_C[np.array(val_vec[:, 1], dtype='int32'), :]), axis = 1)\n",
    "        error_test = test_out - val_vec[:, 2] + mean_inv\n",
    "        test_rmse = LA.norm(error_test)/np.sqrt(len(val_vec))\n",
    "        if epoch % 50 == 0:\n",
    "            # Print information\n",
    "            print('%f th epoch, train RMSE: %f, test RMSE: %f' %(epoch, train_rmse, test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using numpy backend.\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "from tensorly import unfold\n",
    "\n",
    "tensor = scipy.io.loadmat('Guangzhou-data-set/tensor.mat')\n",
    "tensor = tensor['tensor']\n",
    "random_matrix = scipy.io.loadmat('Guangzhou-data-set/random_matrix.mat')\n",
    "random_matrix = random_matrix['random_matrix']\n",
    "random_tensor = scipy.io.loadmat('Guangzhou-data-set/random_tensor.mat')\n",
    "random_tensor = random_tensor['random_tensor']\n",
    "\n",
    "mat = unfold(tensor, 0)\n",
    "missing_rate = 0.4\n",
    "\n",
    "# =============================================================================\n",
    "### Random missing (RM) scenario:\n",
    "### ------------------------------\n",
    "###   missing rate | 0.2 | 0.4 |\n",
    "###   rank         |  80 |  80 |\n",
    "### ------------------------------\n",
    "### Set the RM scenario by:\n",
    "# binary_mat = unfold(np.round(random_tensor + 0.5 - missing_rate), 0)\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "### Non-random missing (NM) scenario:\n",
    "### ------------------------------\n",
    "###   missing rate | 0.2 | 0.4 |\n",
    "###   rank         |  10 |  10 |\n",
    "### ------------------------------\n",
    "### Set the NM scenario by:\n",
    "binary_tensor = np.zeros(tensor.shape)\n",
    "for i1 in range(tensor.shape[0]):\n",
    "    for i2 in range(tensor.shape[1]):\n",
    "        binary_tensor[i1,i2,:] = np.round(random_matrix[i1,i2] + 0.5 - missing_rate)\n",
    "binary_mat = unfold(binary_tensor, 0)\n",
    "# =============================================================================\n",
    "\n",
    "sparse_mat = np.multiply(mat, binary_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.zeros((sparse_mat[sparse_mat > 0].shape[0], 3))\n",
    "start_idx = 0\n",
    "for i in range(sparse_mat.shape[0]):\n",
    "    for t in range(sparse_mat.shape[1]):\n",
    "        if sparse_mat[i, t] > 0:\n",
    "            train_data[start_idx, 0] = i + 1\n",
    "            train_data[start_idx, 1] = t + 1\n",
    "            train_data[start_idx, 2] = sparse_mat[i, t]\n",
    "            start_idx += 1\n",
    "\n",
    "validation_mat = mat\n",
    "validation_mat[sparse_mat > 0] = 0\n",
    "test_data = np.zeros((validation_mat[validation_mat > 0].shape[0], 3))\n",
    "start_idx = 0\n",
    "for i in range(validation_mat.shape[0]):\n",
    "    for t in range(validation_mat.shape[1]):\n",
    "        if validation_mat[i, t] > 0:\n",
    "            test_data[start_idx, 0] = i + 1\n",
    "            test_data[start_idx, 1] = t + 1\n",
    "            test_data[start_idx, 2] = validation_mat[i, t]\n",
    "            start_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.000000 th epoch, train RMSE: 8.015995, test RMSE: 8.061721\n",
      "100.000000 th epoch, train RMSE: 5.160675, test RMSE: 5.180034\n",
      "150.000000 th epoch, train RMSE: 4.839092, test RMSE: 4.894862\n",
      "200.000000 th epoch, train RMSE: 4.651670, test RMSE: 4.754909\n",
      "250.000000 th epoch, train RMSE: 4.497721, test RMSE: 4.653358\n",
      "300.000000 th epoch, train RMSE: 4.366399, test RMSE: 4.578945\n",
      "350.000000 th epoch, train RMSE: 4.251039, test RMSE: 4.523455\n",
      "400.000000 th epoch, train RMSE: 4.154031, test RMSE: 4.486616\n",
      "450.000000 th epoch, train RMSE: 4.090019, test RMSE: 4.473039\n",
      "500.000000 th epoch, train RMSE: 4.052358, test RMSE: 4.469863\n",
      "550.000000 th epoch, train RMSE: 4.030907, test RMSE: 4.471329\n",
      "600.000000 th epoch, train RMSE: 4.016695, test RMSE: 4.470867\n",
      "650.000000 th epoch, train RMSE: 4.007287, test RMSE: 4.472719\n",
      "700.000000 th epoch, train RMSE: 4.001105, test RMSE: 4.475769\n",
      "750.000000 th epoch, train RMSE: 3.996005, test RMSE: 4.474324\n",
      "800.000000 th epoch, train RMSE: 3.990647, test RMSE: 4.475283\n",
      "850.000000 th epoch, train RMSE: 3.984967, test RMSE: 4.481982\n",
      "900.000000 th epoch, train RMSE: 3.981708, test RMSE: 4.475706\n",
      "950.000000 th epoch, train RMSE: 3.978165, test RMSE: 4.477651\n",
      "1000.000000 th epoch, train RMSE: 3.975810, test RMSE: 4.486612\n"
     ]
    }
   ],
   "source": [
    "pmf(train_data, test_data, 10, 0.05, 0.1, 0.8, 1000, 50, 500)\n",
    "### number of features (rank): 10\n",
    "### epsilon: 0.05\n",
    "### _lambda: 0.1\n",
    "### momentum: 0.8\n",
    "### maxepoch: 1000\n",
    "### num_batches: 50\n",
    "### batch_size: 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment results** of missing data imputation using probabilistic matrix factorization (PMF):\n",
    "\n",
    "|  scenario |`rank`|`epsilon`|`_lambda`|`momentum`|`num_batches`|`batch_size`| train_rmse | test_rmse |\n",
    "|:----------|-----:|--------:|--------:|---------:|------------:|-----------:|-----------:|----------:|\n",
    "|**0.4, NM**|   10 |     1.0 |     0.1 |      0.8 |          30 |       1000 |     4.5358 |    4.9376 |\n",
    "|**0.4, NM**|   10 |     0.5 |     0.1 |      0.8 |          30 |       1000 |     4.1773 |    4.6813 |\n",
    "|**0.4, NM**|   10 |     0.1 |     0.1 |      0.8 |          30 |       1000 |     3.9645 |    4.4900 |\n",
    "|**0.4, NM**|   10 |    0.05 |     0.1 |      0.8 |          30 |       1000 |     4.0068 |    4.4679 |\n",
    "|**0.2, NM**|   10 |     0.1 |     0.1 |      0.8 |          30 |       1000 |     4.0253 |    4.3754 |\n",
    "|**0.2, NM**|   10 |    0.05 |     0.1 |      0.8 |          30 |       1000 |     4.0530 |    4.3651 |\n",
    "|**0.2, NM**|   10 |    0.05 |     0.1 |      0.8 |          20 |       2000 |     4.2094 |    4.4133 |\n",
    "|**0.2, NM**|   10 | **0.05**|     0.1 |      0.8 |       **50**|     **500**|     4.0291 | **4.3575**|\n",
    "|**0.4, NM**|   10 | **0.05**|     0.1 |      0.8 |       **50**|     **500**|     3.9758 | **4.4866**|\n",
    "|**0.2, RM**|   80 | **0.05**|     0.1 |      0.8 |       **50**|     **500**|     2.8055 | **4.0909**|\n",
    "|**0.4, RM**|   80 | **0.05**|     0.1 |      0.8 |       **50**|     **500**|     2.4889 | **4.2280**|\n",
    "\n",
    "   > The experiment relies on the *Urban traffic speed data set in Guangzhou, China*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
